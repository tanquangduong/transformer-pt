{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113fed67-4d7c-490e-ab2b-3fb2e6de09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880f3488-8c22-49f3-81f0-72c222ac8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: \n",
    "        # d_model: feature length of token\n",
    "        # h: number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # d_model % num_heads should be zero\n",
    "        assert d_model % h == 0, \"d_model % num_heads should be zero\" \n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # parameter matrix for query W_Q\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # parameter matrix for key W_K\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # parameter matrix for value W_V\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # parameter matrix for output W_O\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query_k, key_k, value_k, d_k, mask=None, dropout=nn.Dropout):\n",
    "        # query_k: [batch_size, h, seq_len, d_k]\n",
    "        # key_k: [batch_size, h, seq_len, d_k]\n",
    "        # value_k: [batch_size, h, seq_len, d_k]\n",
    "        # mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        attention_score = (query_k @ key_k.transpose(-2, -1)) / math.sqrt(d_k) # [batch_size, h, seq_len, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_score = torch.softmax(attention_score, dim=-1) # [batch_size, h, seq_len, seq_len]\n",
    "        attention_score = dropout(attention_score)\n",
    "\n",
    "        return attention_score @ value_k, attention_score # [batch_size, h, seq_len, d_k], [batch_size, h, seq_len, seq_len]\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query: [batch_size, seq_len, d_model]\n",
    "        # key: [batch_size, seq_len, d_model]\n",
    "        # value: [batch_size, seq_len, d_model]\n",
    "        # mask: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "        query_k = self.w_q(query) # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
    "        key_k = self.w_k(key) # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
    "        value_k = self.w_v(value) # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, h, d_k] -> [batch_size, h, seq_len, d_k]\n",
    "        query_k = query_k.view(query_k.shape[0], query_k.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key_k = key_k.view(key_k.shape[0], key_k.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value_k = value_k.view(value_k.shape[0], value_k.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention  \n",
    "        attention, _ = self.attention(query_k, key_k, value_k, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # Concatenate h heads\n",
    "        # [batch_size, h, seq_len, d_k] -> [batch_size, seq_len, h, d_k] -> [batch_size, seq_len, d_model]\n",
    "        attention = attention.transpose(1, 2).contiguous().view(attention.shape[0], -1, self.d_model)\n",
    "\n",
    "        return self.w_o(attention) # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b2a44-87df-4964-a7f7-1c993697b132",
   "metadata": {},
   "source": [
    "## Example to calculate multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce6a1cb-fbc1-4acd-ba45-2584eb19b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions, number of heads, and dropout rate\n",
    "d_model = 512\n",
    "h = 8\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of the MultiHeadAttention class\n",
    "multi_head_attention = MultiHeadAttention(d_model, h, dropout)\n",
    "\n",
    "# Create random tensors to represent a batch of sequences for query, key, and value\n",
    "query = torch.rand(10, 20, d_model)  # batch_size=10, seq_len=20, d_model=512\n",
    "key = torch.rand(10, 20, d_model)  # batch_size=10, seq_len=20, d_model=512\n",
    "value = torch.rand(10, 20, d_model)  # batch_size=10, seq_len=20, d_model=512\n",
    "\n",
    "# Pass the tensors through the multi-head attention layer\n",
    "output = multi_head_attention(query, key, value)\n",
    "\n",
    "print(output.shape)  # Should print: torch.Size([10, 20, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424b4c6-87ea-484d-bacb-86534182de81",
   "metadata": {},
   "source": [
    "## Example to calculate attention and attention score on single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee85631-3764-43be-acd1-a5899f7934ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 6, 64])\n",
      "torch.Size([1, 8, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions\n",
    "d_k = 64\n",
    "\n",
    "# Create an instance of the MultiHeadAttention class\n",
    "multi_head_attention = MultiHeadAttention(d_model=512, h=8, dropout=0.1)\n",
    "\n",
    "# Create random tensors to represent a batch of sequences for query, key, and value\n",
    "query_k = torch.rand(1, 8, 6, d_k)  # batch_size=1, h=8, seq_len=6, d_k=64\n",
    "key_k = torch.rand(1, 8, 6, d_k)  # batch_size=1, h=8, seq_len=6, d_k=64\n",
    "value_k = torch.rand(1, 8, 6, d_k)  # batch_size=1, h=8, seq_len=6, d_k=64\n",
    "\n",
    "# Pass the tensors through the attention method\n",
    "output, attention_score = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(output.shape)  # Should print: torch.Size([10, 8, 20, 64])\n",
    "print(attention_score.shape)  # Should print: torch.Size([10, 8, 20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b39ca42-0787-4287-89e1-de2df2b4c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1626, 0.1937, 0.1655, 0.1733, 0.2434, 0.1725],\n",
      "          [0.1569, 0.2007, 0.1504, 0.1910, 0.2289, 0.1832],\n",
      "          [0.1969, 0.0000, 0.1830, 0.1584, 0.2203, 0.1938],\n",
      "          [0.1353, 0.0000, 0.1941, 0.1564, 0.2266, 0.2024],\n",
      "          [0.1752, 0.1983, 0.1429, 0.1692, 0.2243, 0.0000],\n",
      "          [0.1461, 0.2089, 0.1711, 0.1633, 0.2581, 0.1637]],\n",
      "\n",
      "         [[0.0000, 0.1600, 0.1884, 0.2143, 0.1807, 0.1928],\n",
      "          [0.2084, 0.1580, 0.2117, 0.0000, 0.1749, 0.1757],\n",
      "          [0.1747, 0.1441, 0.2124, 0.2295, 0.1589, 0.1916],\n",
      "          [0.1970, 0.0000, 0.2190, 0.1816, 0.1817, 0.1895],\n",
      "          [0.1533, 0.1618, 0.2093, 0.2504, 0.1704, 0.1659],\n",
      "          [0.1727, 0.1565, 0.0000, 0.2222, 0.0000, 0.1967]],\n",
      "\n",
      "         [[0.1684, 0.1952, 0.1737, 0.2013, 0.1876, 0.1848],\n",
      "          [0.1809, 0.2249, 0.1764, 0.1714, 0.1840, 0.1735],\n",
      "          [0.1814, 0.2028, 0.1689, 0.1806, 0.0000, 0.0000],\n",
      "          [0.2120, 0.2057, 0.1746, 0.1879, 0.1720, 0.0000],\n",
      "          [0.1787, 0.2027, 0.1890, 0.1979, 0.1725, 0.1703],\n",
      "          [0.0000, 0.2201, 0.1553, 0.2035, 0.1722, 0.1590]],\n",
      "\n",
      "         [[0.1991, 0.1814, 0.1367, 0.2493, 0.1710, 0.0000],\n",
      "          [0.2235, 0.1555, 0.1383, 0.2022, 0.2025, 0.0000],\n",
      "          [0.0000, 0.1770, 0.1548, 0.2096, 0.1588, 0.2048],\n",
      "          [0.1806, 0.1560, 0.1548, 0.2344, 0.1642, 0.2211],\n",
      "          [0.1950, 0.1773, 0.0000, 0.0000, 0.1591, 0.1829],\n",
      "          [0.2080, 0.1516, 0.1388, 0.2230, 0.1904, 0.1994]],\n",
      "\n",
      "         [[0.0000, 0.2005, 0.1976, 0.0000, 0.2029, 0.0000],\n",
      "          [0.1709, 0.1838, 0.1874, 0.1935, 0.2036, 0.1720],\n",
      "          [0.1836, 0.1915, 0.1951, 0.1782, 0.0000, 0.1566],\n",
      "          [0.1911, 0.2109, 0.1694, 0.1636, 0.2351, 0.0000],\n",
      "          [0.1860, 0.2136, 0.0000, 0.1637, 0.2106, 0.1337],\n",
      "          [0.1882, 0.1981, 0.0000, 0.1621, 0.0000, 0.1509]],\n",
      "\n",
      "         [[0.0000, 0.2411, 0.0000, 0.1770, 0.1428, 0.1694],\n",
      "          [0.2016, 0.2306, 0.1739, 0.1737, 0.1632, 0.1681],\n",
      "          [0.2045, 0.2199, 0.1731, 0.1661, 0.0000, 0.1925],\n",
      "          [0.2051, 0.2275, 0.1394, 0.1926, 0.1497, 0.0000],\n",
      "          [0.2256, 0.2162, 0.1563, 0.0000, 0.1502, 0.1604],\n",
      "          [0.2003, 0.2103, 0.1706, 0.1768, 0.1658, 0.1873]],\n",
      "\n",
      "         [[0.1863, 0.1669, 0.1823, 0.2114, 0.1923, 0.1719],\n",
      "          [0.1946, 0.1979, 0.1848, 0.1829, 0.1600, 0.1909],\n",
      "          [0.1843, 0.1561, 0.2139, 0.2128, 0.1474, 0.1967],\n",
      "          [0.1985, 0.1960, 0.1963, 0.1828, 0.1601, 0.1774],\n",
      "          [0.1877, 0.1429, 0.1981, 0.0000, 0.1646, 0.1790],\n",
      "          [0.1917, 0.2103, 0.1867, 0.2057, 0.1493, 0.1674]],\n",
      "\n",
      "         [[0.1568, 0.2308, 0.2193, 0.1499, 0.1328, 0.2216],\n",
      "          [0.1605, 0.0000, 0.2336, 0.0000, 0.1566, 0.2095],\n",
      "          [0.1670, 0.1849, 0.2429, 0.0000, 0.1655, 0.0000],\n",
      "          [0.1681, 0.0000, 0.2221, 0.1538, 0.0000, 0.2133],\n",
      "          [0.1876, 0.2169, 0.2069, 0.1450, 0.0000, 0.2069],\n",
      "          [0.1576, 0.2172, 0.2334, 0.1611, 0.1382, 0.2037]]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110e64a-f163-432b-8b75-896013c568bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

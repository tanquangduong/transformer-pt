{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc991a8-29d9-43a8-ae1f-0da38f8830e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8988500-001b-4f95-a770-4a52171ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        base = 10000.0 ** (-1.0 / d_model)\n",
    "        div_term = torch.pow(base, torch.arange(0, d_model, 2).float())\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x): # x(embeded sequence): [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe.requires_grad_(False)\n",
    "        return self.dropout(x) # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24afa07c-baa6-4648-8b47-1fd1c9cc3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 300])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have a sequence length of 50 and we want our embeddings to be of size 300\n",
    "seq_len = 50\n",
    "d_model = 300\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of our PositionalEncoding class\n",
    "pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "# Suppose we have the following embeded batch of 2 sequences (mini-batch size of 2)\n",
    "# Each sequence has 50 words (sequence length of 50)\n",
    "# Each word is represented by a 300-dimensional vector (d_model = 300)\n",
    "x = torch.rand(2, 50, 300)\n",
    "\n",
    "# Pass our sequences through the positional encoding\n",
    "encoder_input = pos_encoding(x)\n",
    "\n",
    "print(encoder_input.shape)  # Should output: torch.Size([2, 50, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05385ea-ea78-4a1d-abf6-c2423a7656db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0031e-01,  1.3413e+00,  9.5727e-01,  ...,  1.8673e+00,\n",
       "           1.3082e-01,  1.1909e+00],\n",
       "         [ 1.3241e+00,  1.4957e+00,  1.7736e+00,  ...,  1.8759e+00,\n",
       "           2.5413e-01,  1.9031e+00],\n",
       "         [ 1.8533e+00,  8.7519e-02,  1.4987e+00,  ...,  1.9938e+00,\n",
       "           1.9857e-01,  1.6533e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -9.0130e-01,  8.5059e-01,  ...,  1.5707e+00,\n",
       "           3.4051e-01,  2.1623e+00],\n",
       "         [ 2.4486e-01,  1.1713e-01,  1.4777e+00,  ...,  1.6192e+00,\n",
       "           3.8397e-01,  2.1147e+00],\n",
       "         [-0.0000e+00,  0.0000e+00,  1.7454e+00,  ...,  2.0527e+00,\n",
       "           2.8044e-01,  2.2023e+00]],\n",
       "\n",
       "        [[ 1.7076e-01,  1.3026e+00,  1.9648e-01,  ...,  1.8652e+00,\n",
       "           2.0502e-03,  0.0000e+00],\n",
       "         [ 1.6273e+00,  1.5889e+00,  1.8088e+00,  ...,  1.7651e+00,\n",
       "           8.3192e-01,  1.4153e+00],\n",
       "         [ 1.6959e+00, -1.0794e-01,  1.8971e+00,  ...,  1.2293e+00,\n",
       "           2.2635e-01,  1.9268e+00],\n",
       "         ...,\n",
       "         [ 9.9376e-01, -0.0000e+00,  7.3634e-01,  ...,  2.1438e+00,\n",
       "           1.0733e+00,  2.1191e+00],\n",
       "         [-8.1942e-01,  3.7617e-01,  1.9715e+00,  ...,  1.4961e+00,\n",
       "           2.0340e-01,  2.0936e+00],\n",
       "         [-3.0631e-01,  0.0000e+00,  2.0046e+00,  ...,  1.5801e+00,\n",
       "           1.1014e-01,  1.7631e+00]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a0dd3-3cdd-4a2e-b7ce-f7e0dd0a36fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7878d427-734a-42ce-a2f1-ee34e1444987",
   "metadata": {},
   "source": [
    "## Usage of Attention without Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f784ddf0-56bc-4c92-babe-406b02875515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 1, 4, 64])\n",
      "Attention Scores Shape: torch.Size([1, 1, 4, 4])\n",
      "Attention Scores:\n",
      "tensor([[[[0.4294, 0.2395, 0.2545, 0.1876],\n",
      "          [0.1694, 0.5158, 0.2546, 0.1713],\n",
      "          [0.2125, 0.3007, 0.4212, 0.1767],\n",
      "          [0.2100, 0.2712, 0.2369, 0.3930]]]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from transformer.layers import MultiHeadAttention\n",
    "# Suppose we have a batch of 1 sequences (mini-batch size of 1)\n",
    "# Each sequence has 4 words (sequence length of 4)\n",
    "# We use 1 attention heads (h = 1) and the dimension of key/query (d_k) is 64\n",
    "batch_size = 1\n",
    "h = 1\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "\n",
    "input_tensor = torch.rand(batch_size, h, seq_len, d_k)\n",
    "query_k = input_tensor.clone()\n",
    "key_k = input_tensor.clone()\n",
    "value_k = input_tensor.clone()\n",
    "\n",
    "# Call the attention function\n",
    "output, attention_scores = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, mask=None, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Attention Scores Shape: {attention_scores.shape}\")\n",
    "print(f\"Attention Scores:\\n{attention_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf76bdc-a4f5-4fd8-ba80-f1005bb6aea3",
   "metadata": {},
   "source": [
    "## Example of Attention with Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da033149-ae04-4912-b115-efd7a73b686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Mask:\n",
      "tensor([[[1, 0, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0]]])\n",
      "\n",
      "Output Shape: torch.Size([1, 1, 4, 64])\n",
      "Attention Scores Shape: torch.Size([1, 1, 4, 4])\n",
      "Attention Scores:\n",
      "tensor([[[[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4744, 0.6367, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6133, 0.4978, 0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from transformer.layers import MultiHeadAttention\n",
    "# Suppose we have a batch of 1 sequences (mini-batch size of 1)\n",
    "# Each sequence has 4 words (sequence length of 4)\n",
    "# We use 1 attention heads (h = 1) and the dimension of key/query (d_k) is 64\n",
    "batch_size = 1\n",
    "h = 1\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "\n",
    "input_tensor = torch.rand(batch_size, h, seq_len, d_k)\n",
    "query_k = input_tensor.clone()\n",
    "key_k = input_tensor.clone()\n",
    "value_k = input_tensor.clone()\n",
    "\n",
    "# Create a mask for values above the diagonal\n",
    "causal_mask = torch.triu(torch.ones((1, 4, 4)), diagonal=1).type(torch.int) == 0\n",
    "padding_mask = torch.tensor([1, 1, 0, 0])\n",
    "decoder_mask = padding_mask & causal_mask\n",
    "\n",
    "# Call the attention function\n",
    "output, attention_scores = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, mask=decoder_mask, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(f\"Decoder Mask:\\n{decoder_mask}\\n\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Attention Scores Shape: {attention_scores.shape}\")\n",
    "print(f\"Attention Scores:\\n{attention_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de28eb5-90fb-40c4-9ccb-a6a6d07580b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

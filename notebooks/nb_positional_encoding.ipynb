{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e6fc9-408c-484e-bc45-1a9b7c4723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from transformer.layers import PositionalEncoding\n",
    "\n",
    "# Suppose we have a sequence length of 50 and we want our embeddings to be of size 300\n",
    "seq_len = 50\n",
    "d_model = 300\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of our PositionalEncoding class\n",
    "pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "# Suppose we have the following embeded batch of 2 sequences (mini-batch size of 2)\n",
    "# Each sequence has 50 words (sequence length of 50)\n",
    "# Each word is represented by a 300-dimensional vector (d_model = 300)\n",
    "x = torch.rand(2, 50, 300)\n",
    "\n",
    "# Pass our sequences through the positional encoding\n",
    "positional_feature = pos_encoding(x)\n",
    "\n",
    "print(positional_feature.shape)  # Should output: torch.Size([2, 50, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05385ea-ea78-4a1d-abf6-c2423a7656db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8999,  1.4140,  0.1214,  ...,  1.9595,  0.4804,  1.5597],\n",
       "         [ 1.3037,  1.2068,  0.9678,  ...,  2.1466,  0.3396,  1.4838],\n",
       "         [ 1.8847, -0.1872,  1.5039,  ...,  0.0000,  0.0000,  1.9104],\n",
       "         ...,\n",
       "         [ 0.5251, -0.6248,  0.8024,  ...,  1.7522,  0.0118,  1.7253],\n",
       "         [-0.7688, -0.5686,  1.3100,  ...,  1.2418,  0.6884,  1.7328],\n",
       "         [-0.0000,  0.7418,  2.0147,  ...,  1.7722,  0.8569,  1.7417]],\n",
       "\n",
       "        [[ 0.0826,  2.1638,  0.0000,  ...,  1.5403,  0.9495,  1.4209],\n",
       "         [ 1.8205,  0.9045,  1.5568,  ...,  1.9860,  0.0000,  1.6836],\n",
       "         [ 1.1355,  0.4651,  1.7790,  ...,  1.7967,  0.6846,  1.5809],\n",
       "         ...,\n",
       "         [ 0.3582, -0.6172,  0.3932,  ...,  1.7091,  0.0462,  1.8987],\n",
       "         [-0.4300, -0.5668,  1.7331,  ...,  2.1554,  0.1925,  1.9529],\n",
       "         [-0.9012,  1.3858,  1.5424,  ...,  1.3540,  0.8139,  2.1820]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a0dd3-3cdd-4a2e-b7ce-f7e0dd0a36fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc991a8-29d9-43a8-ae1f-0da38f8830e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformer.layers import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24afa07c-baa6-4648-8b47-1fd1c9cc3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 300])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have a sequence length of 50 and we want our embeddings to be of size 300\n",
    "seq_len = 50\n",
    "d_model = 300\n",
    "dropout = 0.1\n",
    "\n",
    "# Create an instance of our PositionalEncoding class\n",
    "pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "# Suppose we have the following embeded batch of 2 sequences (mini-batch size of 2)\n",
    "# Each sequence has 50 words (sequence length of 50)\n",
    "# Each word is represented by a 300-dimensional vector (d_model = 300)\n",
    "x = torch.rand(2, 50, 300)\n",
    "\n",
    "# Pass our sequences through the positional encoding\n",
    "encoder_input = pos_encoding(x)\n",
    "\n",
    "print(encoder_input.shape)  # Should output: torch.Size([2, 50, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05385ea-ea78-4a1d-abf6-c2423a7656db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0088,  2.1297,  0.9641,  ...,  2.0796,  0.9944,  1.4668],\n",
       "         [ 1.6213,  0.7972,  1.3638,  ...,  1.8113,  0.3185,  1.6928],\n",
       "         [ 1.0129, -0.0187,  1.9357,  ...,  1.6024,  0.8963,  1.9495],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  1.1079,  ...,  1.1283,  0.1379,  1.6124],\n",
       "         [ 0.0579, -0.5857,  1.9449,  ...,  1.4777,  0.3594,  1.4166],\n",
       "         [-0.0000,  1.2474,  1.2148,  ...,  2.0586,  0.9574,  1.2041]],\n",
       "\n",
       "        [[ 0.4630,  1.6637,  0.2949,  ...,  1.6456,  0.9371,  1.4579],\n",
       "         [ 1.7670,  1.5508,  1.1808,  ...,  1.2683,  0.7567,  0.0000],\n",
       "         [ 1.3473, -0.1814,  1.3890,  ...,  1.7275,  0.9335,  1.2035],\n",
       "         ...,\n",
       "         [ 0.7424, -1.0230,  1.3199,  ...,  2.2171,  0.8667,  2.2195],\n",
       "         [-0.6146, -0.1620,  1.9235,  ...,  1.2249,  0.0151,  1.6427],\n",
       "         [-0.5073,  1.0818,  1.7169,  ...,  0.0000,  0.6923,  0.0000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a0dd3-3cdd-4a2e-b7ce-f7e0dd0a36fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
